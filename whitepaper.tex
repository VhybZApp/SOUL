\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage[margin=1.25in]{geometry}
\title{Society of Meta-Agents: The SOUL Motivation Framework for Specific and Objective Understanding Logic}
\author{Keyvan M. Sadeghi}
\date{\today}

\begin{document}
\maketitle

\section{Literature Review}

We have identified six practical schools of agent motivation:

\begin{itemize}
  \item \textbf{Intrinsic Motivation in RL}: Curiosity and surprise as auxiliary rewards to drive exploration in sparse environments (Schmidhuber, 2010; Pathak et al., 2017).
  \item \textbf{Information-Theoretic Drives}: Maximizing mutual information, novelty, and empowerment signals (Klyubin et al., 2005; Salge et al., 2014; Goertzel, 2024).
  \item \textbf{Competence-Progress Models}: Rewarding measurable learning progress toward self-generated subgoals to form a self-curriculum (Oudeyer \& Kaplan, 2007).
  \item \textbf{Homeostatic/Drive-Reduction}: Minimizing internal variables (e.g., prediction error) via cybernetic drives (Hull, 1943; Friston, 2010).
  \item \textbf{Cognitive Architectures}: Embedding scalar drives into symbolic cycles (Soar's operator preferences; ACT-R's Expected Value of Control; Goertzel, 2024) for precise control updates.
  \item \textbf{Developmental-Robotics Hybrids}: Combining maturational constraints with competence progress to emulate developmental curricula (Lungarella et al., 2003).
\end{itemize}

\section{Mathematical Foundations and Architectural Decisions}

\subsection{The Motivation Vector}
At the heart of the SOUL Motivation Framework is the hidden internal state, the Motivation Vector:
\[
  \mathbf{s}_t = [s_c, s_u, s_h]
\]
where $s_c$ is competence, $s_u$ is novelty/surprise, and $s_h$ is homeostasis. This vector is updated after every interaction and drives all agent actions. In code, it is maintained as a Python dataclass and is never exposed to the LLM.

\subsection{Motivation Vector Updates}
\begin{itemize}
  \item \textbf{Competence Progress:}
  \[
    \Delta_c = p_g(t) - p_g(t-1), \quad s_c \leftarrow \mathrm{proj}_{[0,1]}(s_c + \alpha \Delta_c)
  \]
  where $p_g(t)$ is the agent's measured performance at time $t$, $\alpha > 0$ is a learning rate, and $\mathrm{proj}_{[0,1]}(x) = \min(\max(x, 0), 1)$ is the projection onto $[0,1]$.
  \item \textbf{Novelty/Surprise:}
  \[
    \mathrm{novel}(t) = 1 - \frac{\mathbf{e}(t) \cdot \mu_{t-1}}{\|\mathbf{e}(t)\|\,\|\mu_{t-1}\|}, \quad s_u \leftarrow \mathrm{proj}_{[0,1]}(s_u + \alpha\,\mathrm{novel}(t))
  \]
  where $\mathbf{e}(t)$ is the current context embedding and $\mu_{t-1}$ is the rolling mean. If $\|\mathbf{e}(t)\|=0$ or $\|\mu_{t-1}\|=0$, set $\mathrm{novel}(t) = 1$ by convention.
  \item \textbf{Homeostatic Decay:}
  \[
    s_h \leftarrow (1-\delta)s_h + \delta
  \]
  where $\delta \in (0,1)$, ensuring $s_h$ gently returns to baseline.
\end{itemize}

\subsection{Meta-Graph and Rule Engine}
The agent maintains a symbolic meta-graph $G$ (a directed graph of rewrite rules $R$ and meta-rules $M$). Each node represents a pattern or rule, and edges encode transformations or relationships. In Python, this is implemented with \texttt{networkx.DiGraph}.

\subsection{Thresholded Nudge and Confidence}
At each step, the agent computes a confidence score $C_t$:
\[
  C_t = f_\mathrm{match}(x_t, G, \mathbf{s}_t)
\]
where $f_\mathrm{match}$ is a similarity or density function over meta-graph patterns and the current state (e.g., a softmax-weighted sum). The agent compares $C_t$ to a dynamic threshold $\tau_t$:
\[
  \text{If}\quad C_t \geq \tau_t\quad \text{then nudge; else remain silent (null action)}
\]

\textbf{Note:} All functions such as $\mathrm{proj}_{[0,1]}$, $f_\mathrm{match}$, $\mathrm{Perceive}$, $\mathrm{UpdateMotivation}$, and $\mathrm{HarvestAxiom}$ are defined in the Appendix.

\subsection{Perception--Cognition--Action Loop}
The agent's operation at each time $t$ is:
\begin{align*}
  \text{Perceive:} &\quad y_t = \mathrm{Perceive}(x_t) \\
  \text{Update:} &\quad \mathbf{s}_{t+1} = \mathrm{UpdateMotivation}(\mathbf{s}_t, y_t) \\
  \text{Record:} &\quad G_{t+1} = \mathrm{UpdateMetaGraph}(G_t, x_t, y_t) \\
  \text{Confidence:} &\quad C_{t+1} = f_\mathrm{match}(x_{t+1}, G_{t+1}, \mathbf{s}_{t+1}) \\
  \text{Action:} &\quad a_{t+1} = \begin{cases}
    \mathrm{HarvestAxiom}(G_{t+1}, x_{t+1}) & \text{if } C_{t+1} \geq \tau_{t+1} \\
    \varnothing & \text{otherwise}
  \end{cases}
\end{align*}
\textit{Explanation:} $a_{t+1}$ is either a harvested axiom/rule (to be injected as a nudge) or the null action $\varnothing$ (agent remains silent).
\subsection{Subgoal Discovery}
If $s_c$ stagnates or $s_u$ spikes, the agent auto-discovers new subgoals by clustering novel contexts in $G$ and generating new rules. This enables adaptive exploration.

\subsection{Discrete Generative Core and Error Signals}
Instincts and policies are encoded as rewrite rules in $G$. The agent predicts a distribution $p_t(m)$ over outcomes, observes $q_t(m)$, and computes error:
\[
  e_t = D_{\mathrm{KL}}(q_t\|p_t) = \sum_m q_t(m) \log \frac{q_t(m)}{p_t(m)}
\]
where $q_t$ and $p_t$ are distributions with $\mathrm{supp}(q_t) \subseteq \mathrm{supp}(p_t)$, or $p_t(m)$ is regularized (e.g., $p_t(m) \leftarrow \max(p_t(m), \epsilon)$ for small $\epsilon$).

This error drives reward and learning:
\[
  r^{\mathrm{int}}_t = -e_t, \qquad r^{\mathrm{ep}}_t = \beta \sum_m q_t(m)\log\frac{1}{p_t(m)}, \quad \beta > 0
\]
where $\beta$ is a proportionality constant.

\subsection{Meta-Rule Self-Modification}
Meta-rules $M$ rewrite the rule graph itself:
\[
  m_i \leftarrow \underset{m'\in\mathcal{N}(m_i)}{\arg\min}\;e_t\bigl(R,\,M\setminus\{m_i\}\cup\{m'\}\bigr)
\]
where $\mathcal{N}(m_i)$ denotes the neighborhood of $m_i$ (candidate meta-rule modifications), and $e_t$ is the error as above.

\subsection{Wasserstein Natural Gradient}
Parameterize rule-distribution $p(\xi)$ and update via:
\[
  \xi_{k+1} = \xi_k - h\,G(\xi_k)^{-1}\,\nabla_{\xi}F\bigl(p(\xi_k)\bigr)
\]
where $G(\xi_k)$ is the metric tensor (Laplacian) from the rule graph, $h>0$ is the step size, and $F$ is a differentiable objective (e.g., expected reward or negative error).

\subsection{Neural–Symbolic Hybrid and Memory}
Continuous predictive-coding nets (vision and motor) run beneath the discrete core, exchanging features/actions. Long-term memory is implemented via vector stores (e.g., \texttt{faiss}, \texttt{chromadb}) for retrieval and adaptation.

\subsection{LLM Pre-Prompting and Naturalization}
When the agent nudges, it injects symbolic axioms/rules into the LLM prompt. The LLM is pre-prompted to interpret these in MeTTa or similar syntax and translate their intent into natural language or actions.

\subsection{Genetic Mixing and Policy Sharing}
Hyperparameters $(\alpha,\delta,\tau_c,\tau_u)$ are encoded as arrays and can be evolved via genetic algorithms (e.g., \texttt{DEAP}, \texttt{pygad}), enabling agent societies to mix and share policies and meta-graphs.

\subsection{Concrete Python Mapping}
\begin{itemize}
  \item \textbf{Motivation Vector:} Python \texttt{dataclass} with fields for $s_c$, $s_u$, $s_h$.
  \item \textbf{Rule Graph:} \texttt{networkx.DiGraph} with nodes for rules/meta-rules.
  \item \textbf{Neural Nets:} \texttt{torch.nn.Module} or \texttt{jax} models for predictive coding.
  \item \textbf{Memory:} \texttt{faiss} or \texttt{chromadb} vector store.
  \item \textbf{Hyperparameters:} Numpy array or genetic algorithm chromosome.
\end{itemize}

\section{Null Action and Silent Learning}
\textit{If the confidence $C_t$ does not exceed the threshold $\tau_t$, the agent performs the null action $\varnothing$, i.e., it remains silent and continues to observe, record, and learn without intervening.}

\section{Implementation Guide}

Below are concrete steps, with Python library suggestions.

\begin{itemize}
  \item \textbf{Competence-Progress Core}: Track competence gains $\Delta_c$ on self-generated subgoals, updating $s_c\in[0,1]$ via
  \[
    s_c \leftarrow \mathrm{clip}(s_c + \alpha\,\Delta_c,\,0,1)
  \]
  \item \textbf{Novelty/Surprise Seeding}: Compute novelty as cosine distance of new embedding $\mathbf{e}(t)$ to recent mean $\mu_{t-1}$,
  \[
    \mathrm{novel}(t) = 1 - \frac{\mathbf{e}(t)\cdot\mu_{t-1}}{\|\mathbf{e}(t)\|\,\|\mu_{t-1}\|},\quad
    s_u \leftarrow \mathrm{clip}(s_u + \alpha\,\mathrm{novel}(t),\,0,1)
  \]
  \item \textbf{Homeostatic Decay}: Maintain stability $s_h$ toward 1 via
  \[
    s_h \leftarrow (1-\delta)\,s_h + \delta
  \]
  \item \textbf{Thresholded Nudge Mechanism}: At each step, the agent computes a confidence score $C_t$ based on the match between the current context $x_t$, the meta-graph $G$, and the motivation vector $\mathbf{s}_t$:
  \[
    C_t = f_\mathrm{match}(x_t, G, \mathbf{s}_t)
  \]
  The agent compares $C_t$ to a dynamic threshold $\tau_t$:
  \[
    \text{If}\quad C_t \geq \tau_t\quad \text{then nudge; else remain silent (null action)}
  \]
\end{itemize}
  \textit{Explanation:} $f_\mathrm{match}$ may be a similarity or density function over meta-graph patterns, and $\tau_t$ can be static or adaptively tuned.
\begin{itemize}
  \item \textbf{Perception--Cognition--Action}: As formulated in Section 2.5, the agent computes and thresholds a confidence score to determine whether to nudge or remain silent. This is implemented by evaluating a match function between the current context, meta-graph, and motivation vector, and comparing it to a dynamic threshold.
  \item \textbf{Discrete Generative Core}: Represent “instincts” as a \emph{metagraph} of rewrite rules $R$, inducing a predicted distribution $p_t(m)$ over outcomes. Measure error \cite{Goertzel2024}
  \[
    e_t = D_{\mathrm{KL}}(q_t\|p_t)
  \]
  \item \textbf{Reward Signals}: Combine instrumental $r^{\mathrm{int}}_t=-e_t$ and epistemic $r^{\mathrm{ep}}_t\propto\sum_m q_t(m)\log\tfrac1{p_t(m)}$ to guide local rule edits.\cite{Goertzel2024}
  \item \textbf{Meta-Rule Self-Modification}: Define meta-rules $M$ that pattern-match on $R$ and refactor complex rules. Local meta-update:
  \[
    m_i \leftarrow \underset{m'\in\mathcal{N}(m_i)}{\arg\min}\;e_t\bigl(R,\,M\setminus\{m_i\}\cup\{m'\}\bigr)\quad\cite{Goertzel2024}
  \]
  Meta-rules undergo the same reward-driven selection as object-level rules.
  \item \textbf{Wasserstein Natural Gradient}: Parameterize rule-distribution $p(\xi)$ and update via the optimal-transport natural gradient:\cite{Goertzel2024}
  \[
    \xi_{k+1} = \xi_k - h\,G(\xi_k)^{-1}\,\nabla_{\xi}F\bigl(p(\xi_k)\bigr)
  \]
  with metric tensor $G$ from the measure-dependent Laplacian on the rule graph.
  \item \textbf{Neural–Symbolic Hybrid}: Two continuous predictive-coding nets (vision and motor) run beneath the discrete core (a \emph{metagraph} of self-transforming codelets), passing symbolic features and actions in a closed-loop.\cite{Goertzel2024}
\end{itemize}

\paragraph{Null Action and Silence:}
\textit{If the confidence $C_t$ does not exceed the threshold $\tau_t$, the agent performs the null action $\varnothing$, i.e., it remains silent and continues to observe, record, and learn without intervening.}

\section{Considerations}
While the SOUL Motivation Framework offers a mathematically unified approach to agent motivation, several limitations and open questions remain. First, the framework is currently theoretical and lacks empirical validation; its performance and scalability in real-world or large-scale simulated environments are yet to be demonstrated. The symbolic meta-graph and rule-based components, while interpretable, may face challenges in high-dimensional, noisy, or rapidly changing domains where purely neural or end-to-end learning may be more robust.

Alternative paths for future development include hybridizing the SOUL framework with more advanced neural architectures, such as deep reinforcement learning agents or transformer-based world models, to better handle perception and action in complex environments. Additionally, the meta-rule self-modification and genetic mixing mechanisms could be further explored using evolutionary computation or meta-learning techniques, potentially enabling agents to autonomously discover new motivational drives or adapt to novel tasks.

Finally, integration with large language models (LLMs) and other foundation models presents both opportunities and risks. While LLMs can interpret and naturalize symbolic nudges, ensuring alignment and safety in open-ended interactions remains an open challenge. Careful evaluation and iterative refinement will be required as the framework transitions from theory to practice.

\section{Conclusion}
This guide unifies seminal research, architectural principles, and practical Python tools to implement SOUL’s Motivation Framework. By following it, you’ll build agents that self-modify, learn from surprise and progress, and nudge LLMs with precise, adaptive prompts.

The novel contributions of this work are threefold: (1) the explicit formalization of a motivation vector that integrates competence, novelty, and homeostasis in a unified mathematical framework; (2) the introduction of a symbolic meta-graph and meta-rule self-modification mechanism, enabling agents to adaptively restructure their motivational drives and reasoning patterns; and (3) the practical blueprint for integrating symbolic, neural, and genetic components in a modular, extensible architecture. Together, these elements advance the field by providing both a theoretical foundation and a roadmap for future implementation and experimentation.

\begin{thebibliography}{99}
\bibitem{Goertzel2024} B. Goertzel. Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel, 2024. arXiv:2412.16547 [cs.AI].
\bibitem{Pathak2017} D. Pathak \emph{et al.}, Curiosity-Driven Exploration by Self-Supervised Prediction, 2017.
\bibitem{Salge2014} C. Salge \emph{et al.}, Curiosity-Driven RL Using Information-Gain, 2014.
\bibitem{Friston2010} K. Friston. The Free-Energy Principle: A Unified Brain Theory?, 2010.
\bibitem{Schmidhuber2010} J. Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation, 2010.
\bibitem{Oudeyer2007} P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic Motivation Systems for Autonomous Mental Development, 2007.
\bibitem{Klyubin2005} A. S. Klyubin \emph{et al.}, Empowerment: A Universal Agent-Centric Measure, 2005.
\bibitem{Lungarella2003} M. Lungarella, G. Metta, R. Pfeifer, and G. Sandini. Developmental robotics: A survey, Connection Science, 15(4), 151-190, 2003.
\bibitem{Hull1943} C. L. Hull. \textit{Principles of Behavior: An Introduction to Behavior Theory}. New York: Appleton-Century, 1943.
\end{thebibliography}

\end{document}
