\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{hyperref}
\usepackage[margin=1.25in]{geometry}
\title{Society of Meta-Agents: The SOUL Motivation Framework for Specific and Objective Understanding Logic}
\author{Keyvan M. Sadeghi}
\date{\today}

\begin{document}
\maketitle

\section{Literature Review}

We have identified six practical schools of agent motivation:

\begin{itemize}
  \item \textbf{Intrinsic Motivation in RL}: Curiosity and surprise as auxiliary rewards to drive exploration in sparse environments (Schmidhuber, 2010; Pathak et al., 2017).
  \item \textbf{Information-Theoretic Drives}: Maximizing mutual information, novelty, and empowerment signals (Klyubin et al., 2005; Salge et al., 2014; Goertzel, 2024).
  \item \textbf{Competence-Progress Models}: Rewarding measurable learning progress toward self-generated subgoals to form a self-curriculum (Oudeyer \& Kaplan, 2007).
  \item \textbf{Homeostatic/Drive-Reduction}: Minimizing internal variables (e.g., prediction error) via cybernetic drives (Hull, 1943; Friston, 2010).
  \item \textbf{Cognitive Architectures}: Embedding scalar drives into symbolic cycles (Soar's operator preferences; ACT-R's Expected Value of Control; Goertzel, 2024) for precise control updates.
  \item \textbf{Developmental-Robotics Hybrids}: Combining maturational constraints with competence progress to emulate developmental curricula (Lungarella et al., 2003).
\end{itemize}

\section{Mathematical Foundations and Architectural Decisions}

\subsection{The Motivation Vector}
At the heart of the SOUL Motivation Framework is the hidden internal state, the Motivation Vector:
\[
  \mathbf{s}_t = [s_c, s_u, s_h]
\]
where $s_c$ is competence, $s_u$ is novelty/surprise, and $s_h$ is homeostasis. This vector is updated after every interaction and drives all agent actions. In code, it is maintained as a Python dataclass and is never exposed to the LLM.

\subsection{Motivation Vector Updates}
\begin{itemize}
  \item \textbf{Competence Progress:}
  \[
    \Delta_c = p_g(t) - p_g(t-1), \quad s_c \leftarrow \mathrm{proj}_{[0,1]}(s_c + \alpha \Delta_c)
  \]
  \textit{Explanation:} $p_g(t)$ is the agent's measured performance (competence) at time $t$. $\Delta_c$ is the change in competence since the last step. $s_c$ is updated by adding the scaled change in competence, then projected to stay within $[0,1]$.
  \item \textbf{Novelty/Surprise:}
  \[
    \mathrm{novel}(t) = 1 - \frac{\mathbf{e}(t) \cdot \mu_{t-1}}{\|\mathbf{e}(t)\|\,\|\mu_{t-1}\|}, \quad s_u \leftarrow \mathrm{proj}_{[0,1]}(s_u + \alpha\,\mathrm{novel}(t))
  \]
  \textit{Explanation:} $\mathbf{e}(t)$ is the embedding of the current context; $\mu_{t-1}$ is the mean embedding of past contexts. This computes the cosine similarity between current and past contexts, subtracts from 1, so higher values mean more novelty. $s_u$ is updated by adding the scaled novelty score, projected to $[0,1]$. If either norm is zero, set $\mathrm{novel}(t) = 1$.
  \item \textbf{Homeostatic Decay:}
  \[
    s_h \leftarrow (1-\delta)s_h + \delta
  \]
  \textit{Explanation:} $s_h$ is the homeostatic drive, which gently decays toward a baseline (e.g., 1) at rate $\delta$. This ensures the agent doesn't get stuck at extremes.
\end{itemize}

\subsection{Meta-Graph and Rule Engine}
The agent maintains a symbolic meta-graph $G$ (a directed graph of rewrite rules $R$ and meta-rules $M$). Each node represents a pattern or rule, and edges encode transformations or relationships. In Python, this is implemented with \texttt{networkx.DiGraph}.

\textbf{Discrete Generative Core:}
At each turn, the agent predicts a distribution over rules, compares it to observed outcomes, and computes error using KL divergence:
\[
  e_t = D_{\mathrm{KL}}(q_t\|p_t)
\]
\textit{Explanation:} $p_t$ is the predicted distribution over rules; $q_t$ is the observed distribution. $D_{\mathrm{KL}}$ is the Kullback-Leibler divergence, a measure of how one probability distribution diverges from another. $e_t$ is the error signal used for learning and adaptation.

\subsection{Thresholded Nudge and Confidence}
At each step, the agent computes a confidence score $C_t$:
\[
  C_t = f_\mathrm{match}(x_t, G, \mathbf{s}_t)
\]
\textit{Explanation:} $C_t$ is a confidence score computed by comparing the current context $x_t$ and the agent's state $\mathbf{s}_t$ to the meta-graph $G$ using a similarity or density function $f_\mathrm{match}$ (e.g., a softmax-weighted sum over matching patterns). The agent compares $C_t$ to a dynamic threshold $\tau_t$ to decide whether to intervene.
\[
  \text{If}\quad C_t \geq \tau_t\quad \text{then nudge; else remain silent (null action)}
\]

\textbf{Note:} All functions such as $\mathrm{proj}_{[0,1]}$, $f_\mathrm{match}$, $\mathrm{Perceive}$, $\mathrm{UpdateMotivation}$, and $\mathrm{HarvestAxiom}$ are defined in the Appendix.

\subsection{Perception--Cognition--Action Loop}
The agent's operation at each time $t$ is:
\begin{align*}
  \text{Perceive:} &\quad y_t = \mathrm{Perceive}(x_t) \\
  \text{Update:} &\quad \mathbf{s}_{t+1} = \mathrm{UpdateMotivation}(\mathbf{s}_t, y_t) \\
  \text{Record:} &\quad G_{t+1} = \mathrm{UpdateMetaGraph}(G_t, x_t, y_t) \\
  \text{Confidence:} &\quad C_{t+1} = f_\mathrm{match}(x_{t+1}, G_{t+1}, \mathbf{s}_{t+1}) \\
  \text{Action:} &\quad a_{t+1} = \begin{cases}
    \mathrm{HarvestAxiom}(G_{t+1}, x_{t+1}) & \text{if } C_{t+1} \geq \tau_{t+1} \\
    \varnothing & \text{otherwise}
  \end{cases}
\end{align*}
\textit{Explanation:} $a_{t+1}$ is either a harvested axiom/rule (to be injected as a nudge) or the null action $\varnothing$ (agent remains silent).
\subsection{Subgoal Discovery}
If $s_c$ stagnates or $s_u$ spikes, the agent auto-discovers new subgoals by clustering novel contexts in $G$ and generating new rules. This enables adaptive exploration.

\subsection{Discrete Generative Core and Error Signals}
Instincts and policies are encoded as rewrite rules in $G$. The agent predicts a distribution $p_t(m)$ over outcomes, observes $q_t(m)$, and computes error:
\[
  e_t = D_{\mathrm{KL}}(q_t\|p_t) = \sum_m q_t(m) \log \frac{q_t(m)}{p_t(m)}
\]
\textit{Explanation:} $p_t$ is the predicted distribution over rules; $q_t$ is the observed distribution. $D_{\mathrm{KL}}$ is the Kullback-Leibler divergence, a measure of how one probability distribution diverges from another. $e_t$ is the error signal used for learning and adaptation.
where $q_t$ and $p_t$ are distributions with $\mathrm{supp}(q_t) \subseteq \mathrm{supp}(p_t)$, or $p_t(m)$ is regularized (e.g., $p_t(m) \leftarrow \max(p_t(m), \epsilon)$ for small $\epsilon$).

\subsection{Intrinsic and Episodic Rewards}
The agent receives two types of reward signals:
\begin{itemize}
  \item \textbf{Intrinsic Reward:}
  \[
    r^{\mathrm{int}}_t = -e_t
  \]
  \textit{Explanation:} Internal reward is negative error (the agent is rewarded for reducing surprise).
  \item \textbf{Episodic Reward:}
  \[
    r^{\mathrm{ep}}_t = \beta \sum_m q_t(m)\log\frac{1}{p_t(m)}, \quad \beta > 0
  \]
  \textit{Explanation:} Episodic reward is proportional to the negative log-likelihood of predictions, weighted by $\beta$.
\end{itemize}
$\beta$ is a proportionality constant.

\subsection{Meta-Rule Self-Modification}
Meta-rules $M$ can edit or restructure the meta-graph itself. The agent searches for local edits that minimize error:
\[
  m_i \leftarrow \arg\min_{m' \in \mathcal{N}(m_i)} e_t(R, \{M \setminus m_i\} \cup \{m'\})
\]
\textit{Explanation:} The agent searches for a local change (neighbor $m'$) to a meta-rule $m_i$ that minimizes the error $e_t$. Meta-rules are rules that can rewrite the rule graph itself.
where $\mathcal{N}(m_i)$ denotes the neighborhood of $m_i$ in meta-rule space.

\subsection{Wasserstein Natural Gradient}
Parameterize rule-distribution $p(\xi)$ and update via:
\[
  \xi_{k+1} = \xi_k - h\,G(\xi_k)^{-1}\,\nabla_{\xi}F\bigl(p(\xi_k)\bigr)
\]
\textit{Explanation:} $\xi$ are the parameters of the rule distribution. $G(\xi_k)$ is the Laplacian (a kind of matrix) over the rule graph, encoding its geometry. This is a gradient descent step that respects the structure of the rule space (a "natural gradient").

\subsection{Neuralâ€“Symbolic Hybrid and Memory}
Continuous predictive-coding nets (vision and motor) run beneath the discrete core, exchanging features/actions. Long-term memory is implemented via vector stores (e.g., \texttt{faiss}, \texttt{chromadb}) for retrieval and adaptation.

\subsection{LLM Pre-Prompting and Naturalization}
When the agent nudges, it injects symbolic axioms/rules into the LLM prompt. The LLM is pre-prompted to interpret these in MeTTa or similar syntax and translate their intent into natural language or actions.

\subsection{Genetic Mixing and Policy Sharing}
Hyperparameters $(\alpha,\delta,\tau_c,\tau_u)$ are encoded as arrays and can be evolved via genetic algorithms (e.g., \texttt{DEAP}, \texttt{pygad}), enabling agent societies to mix and share policies and meta-graphs.

\subsection{Concrete Python Mapping}
\begin{itemize}
  \item \textbf{Motivation Vector:} Python \texttt{dataclass} with fields for $s_c$, $s_u$, $s_h$.
  \item \textbf{Rule Graph:} \texttt{networkx.DiGraph} with nodes for rules/meta-rules.
  \item \textbf{Neural Nets:} \texttt{torch.nn.Module} or \texttt{jax} models for predictive coding.
  \item \textbf{Memory:} \texttt{faiss} or \texttt{chromadb} vector store.
  \item \textbf{Hyperparameters:} Numpy array or genetic algorithm chromosome.
\end{itemize}

\section{Null Action and Silent Learning}
\textit{If the confidence $C_t$ does not exceed the threshold $\tau_t$, the agent performs the null action $\varnothing$, i.e., it remains silent and continues to observe, record, and learn without intervening.}

\section{Implementation Guide}

Below are concrete steps, with Python library suggestions.

\subsection{Core Data Structures}
\begin{itemize}
  \item Use \texttt{numpy} for vector operations and embeddings.
  \item Use \texttt{networkx} to represent the metagraph of rewrite rules and compute Laplacians.
  \item Store agent state in a simple \texttt{dataclass}:
  \begin{verbatim}
  from dataclasses import dataclass
  @dataclass
  class State:
      competence: float = 0.0
      curiosity:  float = 0.0
      stability:  float = 1.0
  \end{verbatim}
\end{itemize}

\subsection{Novelty Detector}
\begin{itemize}
  \item Implement rolling mean with \texttt{collections.deque} and cosine distance via \texttt{scipy.spatial.distance.cosine}.
\end{itemize}

\subsection{Rewrite-Rule Engine}
\begin{itemize}
  \item Model rules as Python objects mapping pattern graphs to outputs.
  \item Use \texttt{networkx} pattern-matching or custom graph algorithms for rule application.
  \item Derive $p_t(m)$ by sampling or counting rule firings over stochastic selections (e.g., softmax weights in \texttt{torch}).
\end{itemize}

\subsection{Information-Theoretic Error}
\begin{itemize}
  \item Compute KL divergence with \texttt{scipy.stats.entropy(q, p)}.
\end{itemize}

\subsection{Reward \& State Update Loop}
\begin{enumerate}
  \item Collect feedback score $r(t)$ via environment simulation or user rating.
  \item Update \texttt{State} (Motivation Vector) using the mathematical formulas above, with learning rate \texttt{alpha}.
  \item Compute confidence $C_t$ and compare to threshold $\tau_t$; if $C_t \geq \tau_t$, harvest and inject a nudge (axiom/rule) from the meta-graph.
  \item If nudging, prepend the harvested axiom/rule to the user query and invoke the LLM (e.g., via \texttt{openai} or \texttt{transformers}), relying on a pre-prompt for interpretation.
  \item If not nudging, remain silent and continue to observe, record, and update internal state.
\end{enumerate}
\textbf{Example:} In Python, this loop is implemented as a function that updates the Motivation Vector, computes confidence, and either calls a nudge-injection routine or skips to the next observation.

\subsection{Meta-Rule Implementation}
\begin{itemize}
  \item Represent meta-rules as rules over the rule-graph using \texttt{networkx}.
  \item Define neighborhood $\mathcal{N}(m_i)$ of small metagraph edits.
  \item Apply the meta-rule update formula in your training loop alongside rule edits.
\end{itemize}

\subsection{Natural Gradient Optimization}
\begin{itemize}
  \item Install \texttt{pot} (Python Optimal Transport) for Wasserstein solvers.
  \item Build ground metric matrix $(\omega_{ij})$ from rule-graph distances.
  \item Construct measure-dependent Laplacian via NetworkX weights.
  \item Compute parameter Jacobians with \texttt{autograd} or manual derivatives.
  \item Perform updates using \texttt{numpy.linalg.pinv} for pseudoinverse.
\end{itemize}

\subsection{Continuous Predictive-Coding Nets}
\begin{itemize}
  \item Use \texttt{PyTorch} or \texttt{JAX} to implement NGC-style layers:
  \[
    z \leftarrow z + \beta\bigl(-\gamma z + (E\cdot e)\odot \phi'(z) - e\bigr)
  \]
  \textit{Explanation:} $z$ is the neural state (e.g., latent or hidden activations), $\beta$ is the update step size, $\gamma$ is a decay rate, $E$ is a weight matrix, $e$ is the prediction error, and $\phi'(z)$ is the derivative of the activation function. The update combines decay, error-driven feedback, and nonlinearity to iteratively refine $z$ toward reducing prediction error, as in predictive coding networks.
  \item Leverage \texttt{torch.nn.Module} for vision and arm nets.
  \item Optimize with local Hebbian-like or standard optimizers (SGD) per PC update.
\end{itemize}

\subsection{Genetic Tuning (Optional)}
\begin{itemize}
  \item Represent hyperparameters $(\alpha,\delta,\tau_c,\tau_u)$ as a NumPy array.
  \item Use \texttt{DEAP} or \texttt{pygad} for crossover and mutation over logged performance metrics.
\end{itemize}

\subsection{Vector Stores and Long-Term Memory}
\begin{itemize}
  \item Integrate \texttt{faiss} or \texttt{chromadb} for storing past contexts/embeddings.
\end{itemize}

\paragraph{Null Action and Silence:}
\textit{If the confidence $C_t$ does not exceed the threshold $\tau_t$, the agent performs the null action $\varnothing$, i.e., it remains silent and continues to observe, record, and learn without intervening.}

\section{Considerations}
While the SOUL Motivation Framework offers a mathematically unified approach to agent motivation, several limitations and open questions remain. First, the framework is currently theoretical and lacks empirical validation; its performance and scalability in real-world or large-scale simulated environments are yet to be demonstrated. The symbolic meta-graph and rule-based components, while interpretable, may face challenges in high-dimensional, noisy, or rapidly changing domains where purely neural or end-to-end learning may be more robust.

Alternative paths for future development include hybridizing the SOUL framework with more advanced neural architectures, such as deep reinforcement learning agents or transformer-based world models, to better handle perception and action in complex environments. Additionally, the meta-rule self-modification and genetic mixing mechanisms could be further explored using evolutionary computation or meta-learning techniques, potentially enabling agents to autonomously discover new motivational drives or adapt to novel tasks.

Finally, integration with large language models (LLMs) and other foundation models presents both opportunities and risks. While LLMs can interpret and naturalize symbolic nudges, ensuring alignment and safety in open-ended interactions remains an open challenge. Careful evaluation and iterative refinement will be required as the framework transitions from theory to practice.

\section{Conclusion}
This guide unifies seminal research, architectural principles, and practical Python tools to implement SOULâ€™s Motivation Framework. By following it, youâ€™ll build agents that self-modify, learn from surprise and progress, and nudge LLMs with precise, adaptive prompts.

The novel contributions of this work are threefold: (1) the explicit formalization of a motivation vector that integrates competence, novelty, and homeostasis in a unified mathematical framework; (2) the introduction of a symbolic meta-graph and meta-rule self-modification mechanism, enabling agents to adaptively restructure their motivational drives and reasoning patterns; and (3) the practical blueprint for integrating symbolic, neural, and genetic components in a modular, extensible architecture. Together, these elements advance the field by providing both a theoretical foundation and a roadmap for future implementation and experimentation.

\begin{thebibliography}{99}
\bibitem{Goertzel2024} B. Goertzel. Discrete Active Predictive Coding for Goal-Guided Algorithmic Chemistry as a Potential Cognitive Kernel, 2024. arXiv:2412.16547 [cs.AI].
\bibitem{Pathak2017} D. Pathak \emph{et al.}, Curiosity-Driven Exploration by Self-Supervised Prediction, 2017.
\bibitem{Salge2014} C. Salge \emph{et al.}, Curiosity-Driven RL Using Information-Gain, 2014.
\bibitem{Friston2010} K. Friston. The Free-Energy Principle: A Unified Brain Theory?, 2010.
\bibitem{Schmidhuber2010} J. Schmidhuber. Formal Theory of Creativity, Fun, and Intrinsic Motivation, 2010.
\bibitem{Oudeyer2007} P.-Y. Oudeyer, F. Kaplan, and V. V. Hafner. Intrinsic Motivation Systems for Autonomous Mental Development, 2007.
\bibitem{Klyubin2005} A. S. Klyubin \emph{et al.}, Empowerment: A Universal Agent-Centric Measure, 2005.
\bibitem{Lungarella2003} M. Lungarella, G. Metta, R. Pfeifer, and G. Sandini. Developmental robotics: A survey, Connection Science, 15(4), 151-190, 2003.
\bibitem{Hull1943} C. L. Hull. \textit{Principles of Behavior: An Introduction to Behavior Theory}. New York: Appleton-Century, 1943.
\end{thebibliography}

\end{document}
